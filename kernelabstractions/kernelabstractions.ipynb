{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":CUDA"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose a backend\n",
    "# CUDA, AMD, or CPU\n",
    "\n",
    "# If you have no GPU you can still follow along\n",
    "# You might want to install a Kernel with threads enable\n",
    "# `IJulia.installkernel(\"Julia 1.6.2 Threads\", \"--threads=auto\")` and restart\n",
    "# this notebook with that kernel.\n",
    "\n",
    "const BACKEND = :CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m environment at `~/juliacon21-gpu_workshop/kernelabstractions/CUDAEnv/Project.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(string(BACKEND, \"Env\"))\n",
    "\n",
    "# Install packages\n",
    "# pkg\"add KernelAbstractions, Adapt\"\n",
    "\n",
    "# if BACKEND == :CUDA\n",
    "#     pkg\"add CUDAKernels, CUDA\"\n",
    "# elseif BACKEND == :AMD\n",
    "#     pkg\"add ROCMKernels, AMDGPU\"\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "using KernelAbstractions, Adapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUDADevice"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if BACKEND == :CUDA\n",
    "    using CUDA, CUDAKernels\n",
    "    const ArrayT = CuArray\n",
    "    const Device = CUDADevice\n",
    "elseif BACKEND == :AMD\n",
    "    using AMDGPU, ROCMKernels\n",
    "    const ArrayT = CuArray\n",
    "    const Device = CUDADevice\n",
    "else BACKEND == :CPU\n",
    "    const ArrayT = Array\n",
    "    const Device = CPU\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing your first kernel in KernelAbstractions\n",
    "\n",
    "Let's implement the classic: $z = ax + y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "saxpy! (generic function with 5 methods)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@kernel function saxpy!(z, α, x, y)\n",
    "    I = @index(Global)\n",
    "    @inbounds z[I] = α * x[I] + y[I]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside `@kernel` you have access to a dialect that let's you implement GPU style\n",
    "kernels. One example is the `@index` macro that calulcates a valid index. You can\n",
    "ask for your current `Global`, `Group`, or `Local` index. \n",
    "\n",
    "They are also available in different styles, more about that later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How are indicies derived\n",
    "\n",
    "KernelAbstractions has two concepts used to derive indicies.\n",
    "\n",
    "1. `workgroupsize`\n",
    "2. `ndrange`\n",
    "\n",
    "The `workgroupsize` is a local block of threads that are co-executed. The `ndrange`\n",
    "specifies the global index space. This index space will be subdivided by the `workgroupsize`\n",
    "and each group will be executed in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8,), (16,), KernelAbstractions.NDIteration.NoDynamicCheck())"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndrange = (128,)\n",
    "workgroupsize = (16,)\n",
    "blocks, workgroupsize, dynamic = KernelAbstractions.NDIteration.partition(ndrange, workgroupsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `ndrange` and `workgroupsize` can be of arbitrarily dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8, 256, 512), (16, 1, 1), KernelAbstractions.NDIteration.NoDynamicCheck())"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndrange = (128,256,512)\n",
    "workgroupsize = (16,)\n",
    "blocks, workgroupsize, dynamic = KernelAbstractions.NDIteration.partition(ndrange, workgroupsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launching Kernels\n",
    "\n",
    "Before we launch any kernel we first need to instantiate it for our backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelAbstractions.Kernel{CUDADevice, KernelAbstractions.NDIteration.DynamicSize, KernelAbstractions.NDIteration.DynamicSize, typeof(gpu_saxpy!)}(gpu_saxpy!)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel = saxpy!(Device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we now have a `Kernel` object with some information in the type domain.\n",
    "\n",
    "The first argument is the device used, the second and third contain about information\n",
    "about the launch configuration. Here both are `DynamicSize` meaning none was given.\n",
    "Lastly there is the type of the actual function going to be called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static vs Dynamic launch configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's allocate some data\n",
    "\n",
    "x = adapt(ArrayT, rand(64, 32))\n",
    "y = adapt(ArrayT, rand(64, 32))\n",
    "z = similar(x)\n",
    "nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "    Can not partition kernel!\n\n    You created a dynamically sized kernel, but forgot to provide runtime\n    parameters for the kernel. Either provide them statically if known\n    or dynamically.\n    NDRange(Static):  KernelAbstractions.NDIteration.DynamicSize\n    NDRange(Dynamic): nothing\n    Workgroupsize(Static):  KernelAbstractions.NDIteration.DynamicSize\n    Workgroupsize(Dynamic): nothing\n",
     "output_type": "error",
     "traceback": [
      "    Can not partition kernel!\n\n    You created a dynamically sized kernel, but forgot to provide runtime\n    parameters for the kernel. Either provide them statically if known\n    or dynamically.\n    NDRange(Static):  KernelAbstractions.NDIteration.DynamicSize\n    NDRange(Dynamic): nothing\n    Workgroupsize(Static):  KernelAbstractions.NDIteration.DynamicSize\n    Workgroupsize(Dynamic): nothing\n",
      "",
      "Stacktrace:",
      " [1] error(s::String)",
      "   @ Base ./error.jl:33",
      " [2] partition(kernel::KernelAbstractions.Kernel{CUDADevice, KernelAbstractions.NDIteration.DynamicSize, KernelAbstractions.NDIteration.DynamicSize, typeof(gpu_saxpy!)}, ndrange::Nothing, workgroupsize::Nothing)",
      "   @ KernelAbstractions ~/.julia/packages/KernelAbstractions/8W8KX/src/KernelAbstractions.jl:383",
      " [3] launch_config(kernel::KernelAbstractions.Kernel{CUDADevice, KernelAbstractions.NDIteration.DynamicSize, KernelAbstractions.NDIteration.DynamicSize, typeof(gpu_saxpy!)}, ndrange::Nothing, workgroupsize::Nothing)",
      "   @ CUDAKernels ~/.julia/packages/CUDAKernels/dI2Fl/src/CUDAKernels.jl:172",
      " [4] (::KernelAbstractions.Kernel{CUDADevice, KernelAbstractions.NDIteration.DynamicSize, KernelAbstractions.NDIteration.DynamicSize, typeof(gpu_saxpy!)})(::CuArray{Float64, 2}, ::Vararg{Any, N} where N; ndrange::Nothing, dependencies::CUDAKernels.CudaEvent, workgroupsize::Nothing, progress::Function)",
      "   @ CUDAKernels ~/.julia/packages/CUDAKernels/dI2Fl/src/CUDAKernels.jl:191",
      " [5] (::KernelAbstractions.Kernel{CUDADevice, KernelAbstractions.NDIteration.DynamicSize, KernelAbstractions.NDIteration.DynamicSize, typeof(gpu_saxpy!)})(::CuArray{Float64, 2}, ::Vararg{Any, N} where N)",
      "   @ CUDAKernels ~/.julia/packages/CUDAKernels/dI2Fl/src/CUDAKernels.jl:191",
      " [6] top-level scope",
      "   @ In[19]:1",
      " [7] eval",
      "   @ ./boot.jl:360 [inlined]",
      " [8] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1116"
     ]
    }
   ],
   "source": [
    "kernel(z, 0.01, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUDAKernels.CudaEvent(CuEvent(Ptr{Nothing} @0x0000000006b918c0, CuContext(0x0000000004fbff40, instance cc1d00557c414dc1)))"
      ]
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "kernel(z, 0.01, x, y, ndrange=size(z), workgroupsize=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUDAKernels.CudaEvent(CuEvent(Ptr{Nothing} @0x00000000089ffa20, CuContext(0x0000000005100ac0, instance 6afe723c4151a0c0)))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I can leave the workgroupsize up to the backend.\n",
    "\n",
    "kernel(z, 0.01, x, y, ndrange=size(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I wanted I could have instantiated the kernel with static size information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelAbstractions.Kernel{CUDADevice, KernelAbstractions.NDIteration.StaticSize{(16,)}, KernelAbstractions.NDIteration.DynamicSize, typeof(gpu_saxpy!)}(gpu_saxpy!)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel = saxpy!(Device(), (16,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterspace, dynamic = KernelAbstractions.partition(kernel, size(x), nothing)\n",
    "nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×32 CartesianIndices{2, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}}}:\n",
       " CartesianIndex(1, 1)  CartesianIndex(1, 2)  …  CartesianIndex(1, 32)\n",
       " CartesianIndex(2, 1)  CartesianIndex(2, 2)     CartesianIndex(2, 32)\n",
       " CartesianIndex(3, 1)  CartesianIndex(3, 2)     CartesianIndex(3, 32)\n",
       " CartesianIndex(4, 1)  CartesianIndex(4, 2)     CartesianIndex(4, 32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KernelAbstractions.NDIteration.blocks(iterspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16×1 CartesianIndices{2, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}}}:\n",
       " CartesianIndex(1, 1)\n",
       " CartesianIndex(2, 1)\n",
       " CartesianIndex(3, 1)\n",
       " CartesianIndex(4, 1)\n",
       " CartesianIndex(5, 1)\n",
       " CartesianIndex(6, 1)\n",
       " CartesianIndex(7, 1)\n",
       " CartesianIndex(8, 1)\n",
       " CartesianIndex(9, 1)\n",
       " CartesianIndex(10, 1)\n",
       " CartesianIndex(11, 1)\n",
       " CartesianIndex(12, 1)\n",
       " CartesianIndex(13, 1)\n",
       " CartesianIndex(14, 1)\n",
       " CartesianIndex(15, 1)\n",
       " CartesianIndex(16, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KernelAbstractions.NDIteration.workitems(iterspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelAbstractions.Kernel{CUDADevice, KernelAbstractions.NDIteration.StaticSize{(16,)}, KernelAbstractions.NDIteration.StaticSize{(1024, 32)}, typeof(gpu_saxpy!)}(gpu_saxpy!)"
      ]
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "kernel = saxpy!(Device(), (16,), (1024, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUDAKernels.CudaEvent(CuEvent(Ptr{Nothing} @0x000000001d79e990, CuContext(0x0000000004fbff40, instance cc1d00557c414dc1)))"
      ]
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "# Launching a mismatched kernel should error\n",
    "# Fixme(vchuravy)\n",
    "\n",
    "kernel(z, 0.01, x, y, ndrange=size(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "Note how the previous examples all returned an `<:Event`. KernelAbstractions launches\n",
    "all kernels asynchronously and it is up to the programmer (YOU!) to ensure that kernels\n",
    "are properly synchronized with the host, other kernels, and GPUArrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelAbstractions.Kernel{CUDADevice, KernelAbstractions.NDIteration.DynamicSize, KernelAbstractions.NDIteration.DynamicSize, typeof(gpu_saxpy!)}(gpu_saxpy!)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel = saxpy!(Device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Allocate data\n",
    "x = adapt(ArrayT, rand(64, 32))\n",
    "y = adapt(ArrayT, rand(64, 32))\n",
    "z = similar(x)\n",
    "\n",
    "# Note: CUDA.jl uses asynchronous allocations, and we are moving data from host\n",
    "#       to the device.\n",
    "\n",
    "allocation_event = Event(Device())\n",
    "\n",
    "# 2. Kernel event, kernel needs to synchronize against allocation and data\n",
    "#    movement from above.\n",
    "\n",
    "kernel_event = kernel(z, 0.01, x, y;\n",
    "                      ndrange=size(z), dependencies=allocation_event)\n",
    "\n",
    "# 3.\n",
    "# Scenario A: reading `z` from the host\n",
    "wait(kernel_event)\n",
    "adapt(Array, z)\n",
    "\n",
    "# Scenario B: Using `z` in the next kernel\n",
    "kernel_event = kernel(x, 0.01, z, y;\n",
    "               ndrange=size(z), dependencies=kernel_event)\n",
    "\n",
    "# Note: We need to wait on `x` now\n",
    "\n",
    "# Scenario C: Using `z` as part of GPUArrays\n",
    "wait(Device(), kernel_event)\n",
    "zz = z.^2 # Broadcast expression is dependent on `z`\n",
    "nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies and CUDA task based programming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the memory hierarchy on a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.2 Threads 1.6.2",
   "language": "julia",
   "name": "julia-1.6.2-threads-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}