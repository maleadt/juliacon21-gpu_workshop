{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "# Install packages\n",
    "# pkg\"add KernelAbstractions, Adapt\"\n",
    "\n",
    "# Choose a backend\n",
    "# CUDA:\n",
    "# pkg\"add CUDAKernels, CUDA\"\n",
    "\n",
    "# AMD: \n",
    "# pkg\"add ROCMKernels, AMDGPU\"\n",
    "\n",
    "# If you have no GPU you can still follow along\n",
    "# You might want to install a Kernel with threads enable\n",
    "# `IJulia.installkernel(\"Julia 1.6.2 Threads\", \"--threads=auto\")` and restart\n",
    "# this notebook with that kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "using KernelAbstractions, Adapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":CUDA"
      ]
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "const BACKEND = :CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUDADevice"
      ]
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "if BACKEND == :CUDA\n",
    "    using CUDA, CUDAKernels\n",
    "    const ArrayT = CuArray\n",
    "    const Device = CUDADevice\n",
    "elseif BACKEND == :AMD\n",
    "    using AMDGPU, ROCMKernels\n",
    "    const ArrayT = CuArray\n",
    "    const Device = CUDADevice\n",
    "else BACKEND == :CPU\n",
    "    const ArrayT = Array\n",
    "    const Device = CPU\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing your first kernel in KernelAbstractions\n",
    "\n",
    "Let's implement the classic: $z = ax + y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "saxpy! (generic function with 5 methods)"
      ]
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "@kernel function saxpy!(z, α, x, y)\n",
    "    I = @index(Global)\n",
    "    @inbounds z[I] = α * x[I] + y[I]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside `@kernel` you have access to a dialect that let's you implement GPU style\n",
    "kernels. One example is the `@index` macro that calulcates a valid index. You can\n",
    "ask for your current `Global`, `Group`, or `Local` index. \n",
    "\n",
    "They are also available in different styles, more about that later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How are indicies derived\n",
    "\n",
    "KernelAbstractions has two concepts used to derive indicies.\n",
    "\n",
    "1. `workgroupsize`\n",
    "2. `ndrange`\n",
    "\n",
    "The `workgroupsize` is a local block of threads that are co-executed. The `ndrange`\n",
    "specifies the global index space. This index space will be subdivided by the `workgroupsize`\n",
    "and each group will be executed in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8,), (16,), KernelAbstractions.NDIteration.NoDynamicCheck())"
      ]
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "ndrange = (128,)\n",
    "workgroupsize = (16,)\n",
    "blocks, workgroupsize, dynamic = KernelAbstractions.NDIteration.partition(ndrange, workgroupsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `ndrange` and `workgroupsize` can be of arbitrarily dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8, 256, 512), (16, 1, 1), KernelAbstractions.NDIteration.NoDynamicCheck())"
      ]
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "ndrange = (128,256,512)\n",
    "workgroupsize = (16,)\n",
    "blocks, workgroupsize, dynamic = KernelAbstractions.NDIteration.partition(ndrange, workgroupsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launching Kernels\n",
    "\n",
    "Before we launch any kernel we first need to instantiate it for our backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelAbstractions.Kernel{CUDADevice, KernelAbstractions.NDIteration.DynamicSize, KernelAbstractions.NDIteration.DynamicSize, typeof(gpu_saxpy!)}(gpu_saxpy!)"
      ]
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "kernel = saxpy!(Device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we now have a `Kernel` object with some information in the type domain.\n",
    "\n",
    "The first argument is the device used, the second and third contain about information\n",
    "about the launch configuration. Here both are `DynamicSize` meaning none was given.\n",
    "Lastly there is the type of the actual function going to be called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static vs Dynamic launch configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's allocate some data\n",
    "\n",
    "x = adapt(ArrayT, rand(64, 32))\n",
    "y = adapt(ArrayT, rand(64, 32))\n",
    "z = similar(x)\n",
    "nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.code.notebook.error": {
       "message": "    Can not partition kernel!\n\n    You created a dynamically sized kernel, but forgot to provide runtime\n    parameters for the kernel. Either provide them statically if known\n    or dynamically.\n    NDRange(Static):  KernelAbstractions.NDIteration.DynamicSize\n    NDRange(Dynamic): nothing\n    Workgroupsize(Static):  KernelAbstractions.NDIteration.DynamicSize\n    Workgroupsize(Dynamic): nothing\n",
       "name": "ErrorException",
       "stack": "\nStacktrace:\n  [1] error(s::String)\n    @ Base ./error.jl:33\n  [2] partition(kernel::KernelAbstractions.Kernel{CUDADevice, KernelAbstractions.NDIteration.DynamicSize, KernelAbstractions.NDIteration.DynamicSize, typeof(gpu_saxpy!)}, ndrange::Nothing, workgroupsize::Nothing)\n    @ KernelAbstractions ~/.julia/packages/KernelAbstractions/8W8KX/src/KernelAbstractions.jl:383\n  [3] launch_config(kernel::KernelAbstractions.Kernel{CUDADevice, KernelAbstractions.NDIteration.DynamicSize, KernelAbstractions.NDIteration.DynamicSize, typeof(gpu_saxpy!)}, ndrange::Nothing, workgroupsize::Nothing)\n    @ CUDAKernels ~/.julia/packages/CUDAKernels/dI2Fl/src/CUDAKernels.jl:172\n  [4] (::KernelAbstractions.Kernel{CUDADevice, KernelAbstractions.NDIteration.DynamicSize, KernelAbstractions.NDIteration.DynamicSize, typeof(gpu_saxpy!)})(::CuArray{Float64, 2}, ::Vararg{Any, N} where N; ndrange::Nothing, dependencies::CUDAKernels.CudaEvent, workgroupsize::Nothing, progress::Function)\n    @ CUDAKernels ~/.julia/packages/CUDAKernels/dI2Fl/src/CUDAKernels.jl:191\n  [5] (::KernelAbstractions.Kernel{CUDADevice, KernelAbstractions.NDIteration.DynamicSize, KernelAbstractions.NDIteration.DynamicSize, typeof(gpu_saxpy!)})(::CuArray{Float64, 2}, ::Vararg{Any, N} where N)\n    @ CUDAKernels ~/.julia/packages/CUDAKernels/dI2Fl/src/CUDAKernels.jl:191\n  [6] top-level scope\n    @ FOO:1\n  [7] eval\n    @ ./boot.jl:360 [inlined]\n  [8] include_string(mapexpr::typeof(identity), mod::Module, code::String, filename::String)\n    @ Base ./loading.jl:1116\n  [9] include_string(m::Module, txt::String, fname::String)\n    @ Base ./loading.jl:1126\n [10] #invokelatest#2\n    @ ./essentials.jl:708 [inlined]\n [11] invokelatest\n    @ ./essentials.jl:706 [inlined]\n [12] notebook_runcell_request(conn::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, params::NamedTuple{(:code,), Tuple{String}})\n    @ VSCodeServer ~/.vscode-server/extensions/julialang.language-julia-insider-1.3.13/scripts/packages/VSCodeServer/src/serve_notebook.jl:7\n [13] dispatch_msg(x::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, dispatcher::VSCodeServer.JSONRPC.MsgDispatcher, msg::Dict{String, Any})\n    @ VSCodeServer.JSONRPC ~/.vscode-server/extensions/julialang.language-julia-insider-1.3.13/scripts/packages/JSONRPC/src/typed.jl:67\n [14] serve_notebook(pipename::String; crashreporting_pipename::String)\n    @ VSCodeServer ~/.vscode-server/extensions/julialang.language-julia-insider-1.3.13/scripts/packages/VSCodeServer/src/serve_notebook.jl:82\n [15] top-level scope\n    @ ~/.vscode-server/extensions/julialang.language-julia-insider-1.3.13/scripts/notebook/notebook.jl:10\n [16] include(mod::Module, _path::String)\n    @ Base ./Base.jl:386\n [17] exec_options(opts::Base.JLOptions)\n    @ Base ./client.jl:285\n [18] _start()\n    @ Base ./client.jl:485"
      }
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "kernel(z, 0.01, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUDAKernels.CudaEvent(CuEvent(Ptr{Nothing} @0x0000000006b918c0, CuContext(0x0000000004fbff40, instance cc1d00557c414dc1)))"
      ]
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "kernel(z, 0.01, x, y, ndrange=size(z), workgroupsize=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUDAKernels.CudaEvent(CuEvent(Ptr{Nothing} @0x000000001bcfdfc0, CuContext(0x0000000004fbff40, instance cc1d00557c414dc1)))"
      ]
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "# I can leave the workgroupsize up to the backend.\n",
    "\n",
    "kernel(z, 0.01, x, y, ndrange=size(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I wanted I could have instantiated the kernel with static size information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelAbstractions.Kernel{CUDADevice, KernelAbstractions.NDIteration.StaticSize{(16,)}, KernelAbstractions.NDIteration.DynamicSize, typeof(gpu_saxpy!)}(gpu_saxpy!)"
      ]
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "kernel = saxpy!(Device(), (16,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterspace, dynamic = KernelAbstractions.partition(kernel, size(x), nothing)\n",
    "nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4×32 CartesianIndices{2, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}}}:\n",
       " CartesianIndex(1, 1)  CartesianIndex(1, 2)  …  CartesianIndex(1, 32)\n",
       " CartesianIndex(2, 1)  CartesianIndex(2, 2)     CartesianIndex(2, 32)\n",
       " CartesianIndex(3, 1)  CartesianIndex(3, 2)     CartesianIndex(3, 32)\n",
       " CartesianIndex(4, 1)  CartesianIndex(4, 2)     CartesianIndex(4, 32)"
      ]
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "KernelAbstractions.NDIteration.blocks(iterspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16×1 CartesianIndices{2, Tuple{Base.OneTo{Int64}, Base.OneTo{Int64}}}:\n",
       " CartesianIndex(1, 1)\n",
       " CartesianIndex(2, 1)\n",
       " CartesianIndex(3, 1)\n",
       " CartesianIndex(4, 1)\n",
       " CartesianIndex(5, 1)\n",
       " CartesianIndex(6, 1)\n",
       " CartesianIndex(7, 1)\n",
       " CartesianIndex(8, 1)\n",
       " CartesianIndex(9, 1)\n",
       " CartesianIndex(10, 1)\n",
       " CartesianIndex(11, 1)\n",
       " CartesianIndex(12, 1)\n",
       " CartesianIndex(13, 1)\n",
       " CartesianIndex(14, 1)\n",
       " CartesianIndex(15, 1)\n",
       " CartesianIndex(16, 1)"
      ]
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "KernelAbstractions.NDIteration.workitems(iterspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelAbstractions.Kernel{CUDADevice, KernelAbstractions.NDIteration.StaticSize{(16,)}, KernelAbstractions.NDIteration.StaticSize{(1024, 32)}, typeof(gpu_saxpy!)}(gpu_saxpy!)"
      ]
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "kernel = saxpy!(Device(), (16,), (1024, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUDAKernels.CudaEvent(CuEvent(Ptr{Nothing} @0x000000001d79e990, CuContext(0x0000000004fbff40, instance cc1d00557c414dc1)))"
      ]
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "# Launching a mismatched kernel should error\n",
    "# Fixme(vchuravy)\n",
    "\n",
    "kernel(z, 0.01, x, y, ndrange=size(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "Note how the previous examples all returned an `Event`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies and CUDA task based programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating with GPUArrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the memory hierarchy on a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "name": "julia-0.6"
  },
  "language_info": {
   "name": "julia",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}